{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.analysis import *\n",
    "from whoosh.qparser import *\n",
    "from whoosh import scoring\n",
    "from whoosh import *\n",
    "from whoosh.writing import AsyncWriter\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from Eval_metrics import * #my script .py with the implementation of the eval metrics\n",
    "\n",
    "import pylab \n",
    "from matplotlib import colors as mcolors\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define a function to convert the html file into csv'''\n",
    "def converter():\n",
    "    #path to the html files for the Cranfield_DATASET\n",
    "    path_Cranfield = os.getcwd()+ \"\\part_1\\part_1_1\\Cranfield_DATASET\\DOCUMENTS\\_\" \n",
    "    #path to the html files for the Time_DATASET\n",
    "    path_Time = os.getcwd()+ \"\\part_1\\part_1_1\\Time_DATASET\\DOCUMENTS\\_\" \n",
    "    #initialization of the dataframe for the crainfield csv   \n",
    "    cranfield_df=pd.DataFrame(columns=['ID','title','body']) \n",
    "    #initialization of the dataframe for the time csv\n",
    "    time_df=pd.DataFrame(columns=['ID','body']) \n",
    "    # for each doc in the folder Cranfield_DATASET\n",
    "    for i in range(1,1401): \n",
    "        #define the file name\n",
    "        filename1=path_Cranfield+'_'*5+str(i)+'.html' \n",
    "        #open the file in reading mode\n",
    "        with open(filename1) as f:\n",
    "            content = f.read() \n",
    "        #use a html parser to pick the content of the file \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        # save the title \n",
    "        title=soup.title.string \n",
    "        # save the body\n",
    "        body=soup.body.string \n",
    "        #store into the dataframe the content. Each row of the document df is a .html file\n",
    "        cranfield_df=cranfield_df.append({'ID':i,'title':title,'body':body},ignore_index=True) \n",
    "    \n",
    "    #for each doc in the folder Time_DATASET\n",
    "    for j in range(1,423): \n",
    "        #define the file name\n",
    "        filename2=path_Time+'_'*5+str(j)+'.html' \n",
    "        #open the file in reading mode\n",
    "        with open(filename2) as f:\n",
    "            content = f.read() \n",
    "        #use a html parser to pick the content of the file \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        # save the body\n",
    "        body=soup.body.string \n",
    "        #store into the dataframe the content. Each row of the document df is a .html file\n",
    "        time_df=time_df.append({'ID':j,'body':body},ignore_index=True) \n",
    "    return cranfield_df,time_df\n",
    "\n",
    "\n",
    "# save the doc from Cranfield dataset into csv file format\n",
    "doc_Cran_converted = converter()[0]\n",
    "doc_Cran_converted.to_csv(os.getcwd()+\"\\part_1\\part_1_1\\Cranfield_DATASET\\doc_to_index.csv\")\n",
    "\n",
    "# save the doc from Time dataset into csv file format\n",
    "doc_Time_converted = converter()[1]\n",
    "doc_Time_converted.to_csv(os.getcwd()+\"\\part_1\\part_1_1\\Time_DATASET\\Time_to_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to create the dictionaries from the se and gt'''\n",
    "def create_dict(sr,gt,Q):\n",
    "    dict_se= {} \n",
    "    dict_gt= {} \n",
    "    for i in Q:\n",
    "        #key=Query_id,value=list of document ids from SE result\n",
    "        dict_se[i]=list(sr[sr['Query_id']==i]['Doc_ID'])\n",
    "\n",
    "    for i in Q:\n",
    "        #key=Query_id,value=list of relevant document ids) from ground truth\n",
    "        dict_gt[i]=list(gt[gt['Query_id']==i]['Relevant_Doc_id'])\n",
    "    return dict_se,dict_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DEFINE THE FIRST PART OF THE SOFTWARE FOR THE SE''' \n",
    "def sw_1(analyzer,filename,ds):\n",
    "    if ds == 0:\n",
    "        # creating schema with fields id, title and content\n",
    "        schema = Schema(id=ID(stored=True),title=TEXT(stored=False, analyzer=analyzer),\\\n",
    "                    content=TEXT(stored=False, analyzer=analyzer))\n",
    "        directory_containing_the_index = os.getcwd()+\"\\part_1\\part_1_1\\Cranfield_DATASET\" \n",
    "        # create an empty-index according to the just defined schema in the directory where csv file is\n",
    "        ix = create_in(directory_containing_the_index, schema) \n",
    "        # open the index file\n",
    "        ix = index.open_dir(directory_containing_the_index) #open the index file \n",
    "        writer =  AsyncWriter(ix) #define a writer object to add content to the fields\n",
    "        # fill the index:\n",
    "        ALL_DOCUMENTS_file_name = filename #path of the file \n",
    "        # open the file\n",
    "        in_file = open(ALL_DOCUMENTS_file_name, \"r\", encoding='latin1')\n",
    "        # read it as csv\n",
    "        csv_reader = csv.reader(in_file, delimiter=',')  \n",
    "        # to skip the header: first line contains the name of each field.\n",
    "        csv_reader.__next__()\n",
    "        # for each row in the 'doc_to_index' file \n",
    "        for record in csv_reader: \n",
    "            id0 = record[1] # extract the id doc\n",
    "            title = record[2] # extract the title\n",
    "            content = record[3] # extract the body\n",
    "            # add this new doc into the inverted index according to the schema\n",
    "            writer.add_document(id=id0, content=title+' '+content)\n",
    "        # commit all the operations on the file\n",
    "        writer.commit()\n",
    "        # close the file\n",
    "        in_file.close()\n",
    "    else:\n",
    "        # creating schema with fields id and content\n",
    "        schema1 = Schema(id=ID(stored=True),content=TEXT(stored=False, analyzer=analyzer))\n",
    "        directory_containing_the_index = os.getcwd()+\"\\part_1\\part_1_1\\Time_DATASET\" \n",
    "        # create an empty-index according to the just defined schema in the directory where csv file is\n",
    "        ix1 = create_in(directory_containing_the_index, schema1) \n",
    "        # open the index file\n",
    "        ix1 = index.open_dir(directory_containing_the_index) #open the index file \n",
    "        writer1 =  AsyncWriter(ix) #define a writer object to add content to the fields\n",
    "        # fill the index:\n",
    "        ALL_DOCUMENTS_file_name1 = filename #path of the file \n",
    "        # open the file\n",
    "        in_file = open(ALL_DOCUMENTS_file_name, \"r\", encoding='latin1')\n",
    "        # read it as csv\n",
    "        csv_reader1 = csv.reader(in_file, delimiter=',')  \n",
    "        # to skip the header: first line contains the name of each field.\n",
    "        csv_reader1.__next__()\n",
    "        # for each row in the 'doc_to_index' file \n",
    "        for record in csv_reader1: \n",
    "            id1 = record[1] # extract the id doc\n",
    "            content = record[2] # extract the body\n",
    "            # add this new doc into the inverted index according to the schema\n",
    "            writer.add_document(id=id1, content=' '+content)\n",
    "        # commit all the operations on the file\n",
    "        writer1.commit()\n",
    "        # close the file\n",
    "        in_file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DEFINE THE QUERY ENGINE (THE SECOND PART OF THE SOFTWARE FOR THE SE)'''\n",
    "'''\n",
    "Method that given the input query and given the specific SE configuration returns the results of the search\n",
    "input:\n",
    "    analyzer - selected text analyzer from the whoosh library\n",
    "    score_fun - selected scoring function from the whoosh library\n",
    "    input_query - query that's being used for evaluation\n",
    "    max_number_of_results - maximal number of results that should be retrieved which are equal to the number of relevant documents related to that specific query (which we need for calculating R-precision)\n",
    "output: answer - dataframe with results of the given SE given the query; columns of dataframe: [\"Rank\" , \"Doc_ID\" , \"Score\"]\n",
    "''' \n",
    "def sw_2(analyzer,score_fun,input_query,max_number_of_results,ds):\n",
    "    if ds == 0:\n",
    "        directory_containing_the_index = os.getcwd()+\"\\part_1\\part_1_1\\Cranfield_DATASET\" \n",
    "        # thanks to the ix we can retreive doc of interest for the given SE configurations\n",
    "        ix = index.open_dir(directory_containing_the_index) \n",
    "        # define a QueryParser for parsing the input_query\n",
    "        qp = QueryParser(\"content\", ix.schema)\n",
    "        # apply it on the given query\n",
    "        parsed_query = qp.parse(input_query) \n",
    "        # create a Searcher for the Index with the given scoring function \n",
    "        searcher = ix.searcher(weighting=score_fun) \n",
    "        # store results of the query and limiting max number of results\n",
    "        results = searcher.search(parsed_query,limit=max_number_of_results) \n",
    "        # define a dataframe to store the results \n",
    "        result=pd.DataFrame() \n",
    "        row=pd.DataFrame()\n",
    "        for hit in results:\n",
    "            row=pd.DataFrame([str(hit.rank) , int(hit['id']), str(hit.score)]).T\n",
    "            result=result.append(row)\n",
    "        result.columns=[\"Rank\" , \"Doc_ID\" , \"Score\"] \n",
    "        # the column 'score' contains the values of the scoring function, that we use for having in a quantitative way the relevance of a doc for a particulare query\n",
    "        searcher.close()\n",
    "        return result\n",
    "    else:\n",
    "        directory_containing_the_index = os.getcwd()+\"\\part_1\\part_1_1\\Time_DATASET\" \n",
    "        # thanks to the ix we can retreive doc of interest for the given SE configurations\n",
    "        ix = index.open_dir(directory_containing_the_index) \n",
    "        # define a QueryParser for parsing the input_query\n",
    "        qp = QueryParser(\"content\", ix.schema)\n",
    "        # apply it on the given query\n",
    "        parsed_query = qp.parse(input_query) \n",
    "        # create a Searcher for the Index with the given scoring function \n",
    "        searcher = ix.searcher(weighting=score_fun) \n",
    "        # store results of the query and limiting max number of results\n",
    "        results = searcher.search(parsed_query,limit=max_number_of_results) \n",
    "        # define a dataframe to store the results \n",
    "        result1=pd.DataFrame() \n",
    "        row=pd.DataFrame()\n",
    "        for hit in results:\n",
    "            row=pd.DataFrame([str(hit.rank) , int(hit['id']), str(hit.score)]).T\n",
    "            result=result.append(row)\n",
    "        result1.columns=[\"Rank\" , \"Doc_ID\" , \"Score\"] \n",
    "        # the column 'score' contains the values of the scoring function, that we use for having in a quantitative way the relevance of a doc for a particulare query\n",
    "        searcher.close()\n",
    "        return result1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method that given the specific SE configuration(analyzer,score_fun)\n",
    "executes and returns the results for ALL the queries \n",
    "input:\n",
    "    analyzer - selected text analyzer from the whoosh library\n",
    "    score_fun - selected scoring function from the whoosh library\n",
    "output: result - dataframe with the results of the given SE for ALL the queries; columns of df: [\"Rank\" , \"Doc_ID\" , \"Score\"]\n",
    "''' \n",
    "def executor(analyzer,score_fun,ds):\n",
    "    if ds == 0:\n",
    "        result=pd.DataFrame() # dataframe with the results of the given SE for ALL the queries; \n",
    "        tmp=pd.DataFrame() #tmp dataframe \n",
    "        # open the file with all the queries\n",
    "        Queries_file=os.getcwd()+\"\\part_1\\part_1_1\\Cranfield_DATASET\\cran_Queries.tsv\"\n",
    "        Queries=pd.read_csv(Queries_file,sep='\\t')\n",
    "        # open the file of the GT\n",
    "        gt=pd.read_csv(os.getcwd()+\"\\part_1\\part_1_1\\Cranfield_DATASET\\cran_Ground_Truth.tsv\", sep='\\t') \n",
    "        #define a list with the unique query ids\n",
    "\n",
    "        Q=list(gt['Query_id'].unique()) \n",
    "        dq={} #key=Query_id, value=number of relevant documents related to that query_id\n",
    "        for i in Q: # for each query_id\n",
    "            dq[i]=len(list(gt[gt['Query_id']==i]['Relevant_Doc_id']))\n",
    "\n",
    "        file_toindex=os.getcwd()+\"\\part_1\\part_1_1\\Cranfield_DATASET\\doc_to_index.csv\"\n",
    "        # invoke the function to create the schema and to store the index file based on the retrieved 'doc_to_index.csv' file  \n",
    "        sw_1(analyzer,file_toindex,0)\n",
    "        # for each index in the query set\n",
    "        for i in Q:\n",
    "            # store the number of relevant documents related to the specific input query\n",
    "            max_number_of_results_1q=dq[i] #count_of_vals(dq,i)\n",
    "            if max_number_of_results_1q==0:\n",
    "                max_number_of_results_1q=1\n",
    "            # invoke the function that,given the input query and given the specific SE configuration,\n",
    "            # returns the results of the search and store it into a tmp dataframe\n",
    "            tmp=sw_2(analyzer,score_fun,list(Queries[Queries['Query_ID']==i]['Query'])[0],max_number_of_results_1q,0)\n",
    "            tmp['Query_id']=i\n",
    "            result=result.append(tmp) #then for each query add it to the result dataframe \n",
    "        return result\n",
    "    else:\n",
    "        result1=pd.DataFrame() # dataframe with the results of the given SE for ALL the queries; \n",
    "        tmp1=pd.DataFrame() #tmp dataframe \n",
    "        # open the file with all the queries\n",
    "        Queries_file1=os.getcwd()+\"\\part_1\\part_1_1\\Time_DATASET\\time_Queries.tsv\"\n",
    "        Queries1=pd.read_csv(Queries_file1,sep='\\t')\n",
    "        # open the file of the GT\n",
    "        gt1=pd.read_csv(os.getcwd()+\"\\part_1\\part_1_1\\Time_DATASET\\time_Ground_Truth.tsv\", sep='\\t') \n",
    "        #define a list with the unique query ids\n",
    "\n",
    "        Q1=list(gt1['Query_id'].unique()) \n",
    "        dq1={} #key=Query_id, value=number of relevant documents related to that query_id\n",
    "        for i in Q1: # for each query_id\n",
    "            dq1[i]=len(list(gt1[gt1['Query_id']==i]['Relevant_Doc_id']))\n",
    "\n",
    "        file_toindex1=os.getcwd()+\"\\part_1\\part_1_1\\Time_DATASET\\Time_to_index.csv\"\n",
    "        # invoke the function to create the schema and to store the index file based on the retrieved 'doc_to_index.csv' file  \n",
    "        sw_1(analyzer,file_toindex,1)\n",
    "        # for each index in the query set\n",
    "        for i in Q1:\n",
    "            # store the number of relevant documents related to the specific input query\n",
    "            max_number_of_results_1q=dq1[i] #count_of_vals(dq,i)\n",
    "            if max_number_of_results_1q==0:\n",
    "                max_number_of_results_1q=1\n",
    "            # invoke the function that,given the input query and given the specific SE configuration,\n",
    "            # returns the results of the search and store it into a tmp dataframe\n",
    "            tmp1=sw_2(analyzer,score_fun,list(Queries[Queries['Query_ID']==i]['Query'])[0],max_number_of_results_1q,1)\n",
    "            tmp1['Query_id']=i\n",
    "            result1=result1.append(tmp) #then for each query add it to the result dataframe \n",
    "        return result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine(ds):\n",
    "    if ds == 0:\n",
    "        # open the ground truth\n",
    "        gt=pd.read_csv(os.getcwd()+\"\\part_1\\part_1_1\\Cranfield_DATASET\\cran_Ground_Truth.tsv\", sep='\\t')\n",
    "        list_mrr=[] # to store the MRR values for each SE configuration \n",
    "        # define the scoring functions TO CHANGE \n",
    "        score_functions = [scoring.TF_IDF(),scoring.BM25F(B=0.75, content_B=1.0, K1=1.5)]\n",
    "        # define the text analyzers\n",
    "        analyzers = [StemmingAnalyzer(),RegexAnalyzer(),FancyAnalyzer(),LanguageAnalyzer('en')]\n",
    "        #combinations for every chosen analyzer with every chosen scoring function\n",
    "        num_analyzers = len(analyzers)\n",
    "        num_score_fun = len(score_functions)\n",
    "        i=1\n",
    "        sel_ana=['StemmingAnalyzer()','RegexAnalyzer()','FancyAnalyzer()','LanguageAnalyzer()']#text which will be used for graph and for mrr table\n",
    "        scor_func=[' TF_IDF',' BM25F']\n",
    "        for x in range(num_analyzers):\n",
    "            for y in range(num_score_fun):\n",
    "                print(sel_ana[x]+scor_func[y])\n",
    "                # execute queries with the chosen configuration\n",
    "                sr_1=executor(analyzers[x],score_functions[y],0) \n",
    "                #save results of the search engine\n",
    "                sr_1.to_csv(os.getcwd()+\"\\part_1\\part_1_1\\Cranfield_DATASET\"+str(i)+\".csv\",index=False) \n",
    "                #compute the MRR \n",
    "                list_mrr.append((sel_ana[x]+scor_func[y],MRR(sr_1,gt))) \n",
    "                i+=1\n",
    "        # save into a table with MRR evaluation for every search engine configuration \n",
    "        mrrs=pd.DataFrame(list_mrr)\n",
    "        mrrs.to_csv(os.getcwd()+\"\\part_1\\part_1_1\\Cranfield_DATASET\\mrr.csv\", index=False) #store MRR table\n",
    "    else:\n",
    "        # open the ground truth\n",
    "        #ERRORE NEL LEGGERE IL FILE -> CHECK TUTTA LA PARTE DEL TIME DATASET\n",
    "        gt1=pd.read_csv(os.getcwd()+\"\\part_1\\part_1_1\\Time_DATASET\\time_Ground_Truth.tsv\",sep='\\t')\n",
    "        list_mrr1=[] # to store the MRR values for each SE configuration \n",
    "        # define the scoring functions TO CHANGE \n",
    "        score_functions = [scoring.TF_IDF(),scoring.BM25F(B=0.75, content_B=1.0, K1=1.5)]\n",
    "        # define the text analyzers\n",
    "        analyzers = [StemmingAnalyzer(),RegexAnalyzer(),FancyAnalyzer(),LanguageAnalyzer('en')]\n",
    "        #combinations for every chosen analyzer with every chosen scoring function\n",
    "        num_analyzers = len(analyzers)\n",
    "        num_score_fun = len(score_functions)\n",
    "        i=1\n",
    "        sel_ana=['StemmingAnalyzer()','RegexAnalyzer()','FancyAnalyzer()','LanguageAnalyzer()']#text which will be used for graph and for mrr table\n",
    "        scor_func=[' TF_IDF',' BM25F']\n",
    "        for x in range(num_analyzers):\n",
    "            for y in range(num_score_fun):\n",
    "                print(sel_ana[x]+scor_func[y])\n",
    "                # execute queries with the chosen configuration\n",
    "                sr_1=executor(analyzers[x],score_functions[y],1) \n",
    "                #save results of the search engine\n",
    "                sr_1.to_csv(os.getcwd()+\"\\part_1\\part_1_1\\Time_DATASET\"+str(i)+\".csv\",index=False) \n",
    "                #compute the MRR \n",
    "                list_mrr1.append((sel_ana[x]+scor_func[y],MRR(sr_1,gt1))) \n",
    "                i+=1\n",
    "        # save into a table with MRR evaluation for every search engine configuration \n",
    "        mrrs=pd.DataFrame(list_mrr1)\n",
    "        mrrs.to_csv(os.getcwd()+\"\\part_1\\part_1_1\\Time_DATASET\\mrr.csv\", index=False) #store MRR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exec the search engine with the different configurations for the Cranfield dataset\n",
    "cranfield_ds = 0\n",
    "search_engine(cranfield_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTROLLARE PERCHE' NON LEGGE BENE IL FILE E CHECK DI TUTTA LA PARTE SUL TIME DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # exec the search engine with the different configurations for the Time dataset\n",
    "# time_ds = 1\n",
    "# search_engine(time_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_ana=['StemmingAnalyzer()','SimpleAnalyzer()','StandardAnalyzer()','RegexAnalyzer()','FancyAnalyzer()','NgramAnalyzer(5)','KeywordAnalyzer()','LanguageAnalyzer()']#text which will be used for graph and for mrr table\n",
    "scor_func=['TF_IDF',' Frequency',' BM25F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
